# OpenAI API

Comprehensive implementation guide for building a production-ready
OpenAI-compatible LLM endpoint using Cloudflare Workers, AI SDK 5, OpenRouter,
and PostHog analytics.

## üìã **Table of Contents**

1.  [üéØ Quick Start](#-quick-start)
2.  [üèóÔ∏è Architecture Overview](#%EF%B8%8F-architecture-overview)
3.  [ü§ñ AI Integration & Analytics](#-ai-integration--analytics)
4.  [üîí Security & Performance](#-security--performance)
5.  [‚öôÔ∏è Environment Setup](#%EF%B8%8F-environment-setup)
6.  [üöÄ API Implementation](#-api-implementation)
7.  [üß™ Testing Strategy](#-testing-strategy)
8.  [üì¶ Deployment Guide](#-deployment-guide)
9.  [üîß Troubleshooting](#-troubleshooting)
10. [üìö Additional Resources](#-additional-resources)

## üéØ **Quick Start**

### Prerequisites

- Cloudflare account with Workers, D1, and KV enabled
- OpenRouter API key with model access
- PostHog project with LLM observability
- Bun runtime environment

### Installation

```sh
# Clone and setup
git clone https://github.com/WeMake-AI/platform.git
cd platform/workers/openai-api
bun install

# Configure environment
cp .dev.vars.example .dev.vars
# Edit .dev.vars with your API keys

# Start development server
bun run dev
```

### Package Configuration

```json
// package.json
{
  "name": "@wemake/openai-api-worker",
  "version": "1.0.0",
  "description": "OpenAI-compatible API endpoint using Cloudflare Workers",
  "main": "src/index.ts",
  "scripts": {
    "dev": "wrangler dev",
    "build": "wrangler deploy --dry-run",
    "deploy:staging": "wrangler deploy --env staging",
    "deploy:production": "wrangler deploy --env production",
    "test": "vitest run",
    "test:watch": "vitest",
    "test:integration": "vitest run --config vitest.integration.config.ts",
    "test:load": "k6 run tests/load.test.ts",
    "db:migrate": "wrangler d1 execute openai-api --file=./schema.sql",
    "db:migrate:staging": "wrangler d1 execute openai-api-staging --file=./schema.sql",
    "db:migrate:production": "wrangler d1 execute openai-api-prod --file=./schema.sql",
    "lint": "eslint src --ext .ts,.tsx",
    "type-check": "tsc --noEmit"
  },
  "dependencies": {
    "hono": "^4.0.0",
    "ai": "^5.0.0", // latest SDK v5 (5.0.8)
    "@ai-sdk/openai": "^2.0.0", // latest adapter (2.0.7)
    "posthog-node": "^4.0.0",
    "zod": "^3.22.0"
  },
  "devDependencies": {
    "@cloudflare/workers-types": "^4.20240000.0",
    "@types/node": "^20.0.0",
    "typescript": "^5.0.0",
    "vitest": "^1.0.0",
    "wrangler": "^3.0.0",
    "eslint": "^8.0.0",
    "@typescript-eslint/eslint-plugin": "^6.0.0",
    "@typescript-eslint/parser": "^6.0.0"
  },
  "engines": {
    "node": ">=18.0.0"
  }
}
```

### Immediate Next Steps

- [ ] Configure OpenRouter API integration and model access
- [ ] Set up PostHog LLM observability for conversation tracking
- [ ] Configure Cloudflare AI Gateway for caching and analytics
- [ ] Document authentication flow diagrams
- [ ] Specify rate limiting tiers and quotas
- [ ] Create API endpoint schemas and validation rules

## üèóÔ∏è **Architecture Overview**

### Core Components

- **Request/Response Handling**: OpenAI-compatible API endpoints
- **Authentication**: API key validation and user management
- **Rate Limiting**: Per-user quotas with KV storage
- **Logging**: Comprehensive observability with PostHog
- **Model Routing**: Intelligent OpenRouter integration

### API Endpoints

- `/v1/chat/completions` - Chat completions with streaming support
- `/v1/completions` - Text completions (legacy)
- `/v1/models` - Available model listing
- `/health` - System health checks
- `/usage` - Usage analytics and billing

## ü§ñ **AI Integration & Analytics**

### AI SDK 5 Integration Strategy

#### Core Setup

```typescript
// Install dependencies
// bun add ai @ai-sdk/openai

// Configure AI SDK with OpenRouter
import { openai } from "@ai-sdk/openai";
import { generateText, streamText } from "ai";

// Model configuration
const model = openrouter("anthropic/claude-3.5-sonnet");
export function createModel(env: Env, id = "anthropic/claude-3.5-sonnet") {
  const client = openai({
    apiKey: env.OPENROUTER_API_KEY,
    baseURL: "https://openrouter.ai/api/v1"
  });
  return client(id);
}
```

#### Streaming Implementation

```typescript
// Cloudflare Worker streaming endpoint
export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    const { messages } = await request.json();

    const result = await streamText({
      model: createModel(env, "anthropic/claude-3.5-sonnet"),
      messages,
      temperature: 0.7,
      maxTokens: 4096
    });

    return result.toAIStreamResponse();
  }
};
```

#### Tool Calling Support

```typescript
import { z } from "zod";

const tools = {
  weather: {
    description: "Get weather information",
    parameters: z.object({
      location: z.string().describe("The city name")
    }),
    execute: async ({ location }) => {
      // Weather API call
      return { temperature: 22, condition: "sunny" };
    }
  }
};

const result = await generateText({
  model,
  messages,
  tools,
  toolChoice: "auto"
});
```

### PostHog LLM Observability

#### Setup and Configuration

```typescript
// Install PostHog
// bun add posthog-js posthog-node

import { PostHog } from "posthog-node";

// Initialize PostHog
const posthog = new PostHog(env.POSTHOG_API_KEY, {
  host: "https://app.posthog.com",
  flushAt: 1,
  flushInterval: 0
});
```

#### LLM Event Tracking

```typescript
// Track LLM generation events
async function trackLLMGeneration({
  userId,
  input,
  output,
  model,
  tokens,
  latency,
  cost
}: LLMGenerationEvent) {
  await posthog.capture({
    distinctId: userId,
    event: "$ai_generation",
    properties: {
      $ai_input: input,
      $ai_output: output,
      $ai_model: model,
      $ai_input_tokens: tokens.input,
      $ai_output_tokens: tokens.output,
      $ai_latency: latency,
      $ai_cost: cost,
      $ai_provider: "openrouter"
    }
  });
}
```

#### Conversation Tracking

```typescript
// Track conversation flows
async function trackConversation({
  conversationId,
  userId,
  messages,
  metadata
}: ConversationEvent) {
  await posthog.capture({
    distinctId: userId,
    event: "$ai_conversation",
    properties: {
      conversation_id: conversationId,
      message_count: messages.length,
      conversation_length: messages.reduce(
        (acc, msg) => acc + msg.content.length,
        0
      ),
      ...metadata
    }
  });
}
```

#### Privacy Mode Implementation

```typescript
// Privacy-aware tracking
const trackWithPrivacy = (event: any, privacyMode: boolean = false) => {
  if (privacyMode) {
    // Exclude sensitive data
    const { $ai_input, $ai_output, ...safeProperties } = event.properties;
    return posthog.capture({
      ...event,
      properties: {
        ...safeProperties,
        $ai_input_length: event.properties.$ai_input?.length || 0,
        $ai_output_length: event.properties.$ai_output?.length || 0
      }
    });
  }
  return posthog.capture(event);
};
```

### OpenRouter Integration

#### Authentication and Headers

```typescript
// OpenRouter API configuration
const OPENROUTER_CONFIG = {
  baseURL: "https://openrouter.ai/api/v1",
  headers: {
    Authorization: `Bearer ${env.OPENROUTER_API_KEY}`,
    "HTTP-Referer": "https://wemake.cx",
    "X-Title": "WeMake AI Platform",
    "Content-Type": "application/json"
  }
};
```

#### Model Selection and Fallbacks

```typescript
// Intelligent model routing
const MODEL_TIERS = {
  premium: ["anthropic/claude-3.5-sonnet", "openai/gpt-4-turbo"],
  standard: ["anthropic/claude-3-haiku", "openai/gpt-3.5-turbo"],
  budget: ["meta-llama/llama-3-8b-instruct", "mistralai/mistral-7b-instruct"]
};

async function selectModel(
  tier: keyof typeof MODEL_TIERS,
  fallback: boolean = true
) {
  const models = MODEL_TIERS[tier];

  // Fetch the full model list once, with auth
  const res = await fetch("https://openrouter.ai/api/v1/models", {
    headers: { Authorization: `Bearer ${env.OPENROUTER_API_KEY}` }
  });
  if (!res.ok) throw new Error(`Failed to list models: ${res.status}`);
  const { data } = await res.json();
  const ids = new Set(data.map((m: any) => m.id));

  // Return the first available model in the desired tier
  for (const m of models) {
    if (ids.has(m)) return m;
  }

  // Fallback to budget tier if none found
  if (fallback && tier !== "budget") {
    return selectModel("budget", false);
  }

  throw new Error("No available models");
}
```

#### Streaming with Error Handling

```typescript
// Robust streaming implementation
async function streamWithFallback(messages: any[], options: any = {}) {
  const model = await selectModel(options.tier || "standard");

  try {
    const response = await fetch(
      "https://openrouter.ai/api/v1/chat/completions",
      {
        method: "POST",
        headers: OPENROUTER_CONFIG.headers,
        body: JSON.stringify({
          model,
          messages,
          stream: true,
          temperature: options.temperature || 0.7,
          max_tokens: options.maxTokens || 4096
        })
      }
    );

    if (!response.ok) {
      throw new Error(`OpenRouter API error: ${response.status}`);
    }

    return response;
  } catch (error) {
    // Implement fallback logic
    console.error("Primary model failed, trying fallback:", error);
    const fallbackModel = await selectModel("budget");

    return fetch("https://openrouter.ai/api/v1/chat/completions", {
      method: "POST",
      headers: OPENROUTER_CONFIG.headers,
      body: JSON.stringify({
        model: fallbackModel,
        messages,
        stream: true,
        temperature: options.temperature || 0.7,
        max_tokens: options.maxTokens || 2048 // Reduced for budget tier
      })
    });
  }
}
```

#### Cost Tracking and Optimization

```typescript
// Cost calculation and tracking
interface ModelPricing {
  prompt: number;
  completion: number;
}

const MODEL_PRICING: Record<string, ModelPricing> = {
  "anthropic/claude-3.5-sonnet": { prompt: 0.003, completion: 0.015 },
  "openai/gpt-4-turbo": { prompt: 0.01, completion: 0.03 },
  "anthropic/claude-3-haiku": { prompt: 0.00025, completion: 0.00125 }
};

function calculateCost(
  model: string,
  inputTokens: number,
  outputTokens: number
): number {
  const pricing = MODEL_PRICING[model];
  if (!pricing) return 0;

  return (
    (inputTokens / 1000) * pricing.prompt +
    (outputTokens / 1000) * pricing.completion
  );
}

// Usage tracking
async function trackUsage({
  model,
  inputTokens,
  outputTokens,
  userId,
  requestId
}: UsageEvent) {
  const cost = calculateCost(model, inputTokens, outputTokens);

  // Store in analytics
  await posthog.capture({
    distinctId: userId,
    event: "$ai_usage",
    properties: {
      model,
      input_tokens: inputTokens,
      output_tokens: outputTokens,
      total_tokens: inputTokens + outputTokens,
      cost,
      request_id: requestId
    }
  });

async function trackUsage(env: Env, { model, inputTokens, outputTokens, userId, requestId }: UsageEvent) {
  const cost = calculateCost(model, inputTokens, outputTokens);
  // Store in database for billing
  await env.DB.prepare(
    `
    INSERT INTO usage_logs (user_id, model, input_tokens, output_tokens, cost, created_at)
    VALUES (?, ?, ?, ?, ?, ?)
  `
  )
    .bind(
      userId,
      model,
      inputTokens,
      outputTokens,
      cost,
      new Date().toISOString()
    )
    .run();
}
```

- **Cloudflare AI Gateway**: Implemented AI gateway for enhanced caching,
  analytics, rate limiting, and request routing with real-time monitoring
  capabilities

**OpenRouter Integration Strategy**:

1. Single provider integration through OpenRouter's unified API
2. Access to multiple models (GPT, Claude, Llama, etc.) via OpenRouter
3. Model selection based on OpenRouter's routing capabilities
4. Simplified authentication and billing through OpenRouter
5. PostHog LLM observability for comprehensive conversation tracking
6. Cloudflare AI Gateway for enhanced performance and monitoring

## üîí **Security & Performance**

- **Security and Rate Limiting**: Implemented API key/JWT authentication,
  sliding window rate limiting with Cloudflare KV, request validation using Zod
  schemas, and OWASP compliance
- **Monitoring and Observability**: Designed structured logging, performance
  metrics, error tracking, real-time alerting, and operational dashboards
- **LLM-Specific Observability**: PostHog LLM observability captures every
  conversation, model performance, cost analysis, and latency tracking
- **AI Gateway Analytics**: Cloudflare AI Gateway provides caching, request
  analytics, rate limiting, and real-time monitoring for AI workloads

## ‚öôÔ∏è **Environment Setup**

### Environment Variables

```sh
# .dev.vars (local development)
OPENROUTER_API_KEY=your_openrouter_api_key
POSTHOG_API_KEY=your_posthog_project_api_key
POSTHOG_HOST=https://app.posthog.com
ALLOWED_ORIGINS=http://localhost:3000,https://wemake.cx
RATE_LIMIT_PER_MINUTE=60
MAX_TOKENS_DEFAULT=4096
DEFAULT_MODEL=anthropic/claude-3.5-sonnet

# Cloudflare Workers secrets (production)
wrangler secret put OPENROUTER_API_KEY
wrangler secret put POSTHOG_API_KEY
```

### Wrangler Configuration

```toml
# wrangler.toml
name = "openai-api-worker"
main = "src/index.ts"
compatibility_date = "2024-01-15"
compatibility_flags = ["nodejs_compat"]

[env.production]
name = "openai-api-worker-prod"
route = "api.wemake.cx/v1/*"

[env.staging]
name = "openai-api-worker-staging"
route = "api-staging.wemake.cx/v1/*"

[[env.production.d1_databases]]
binding = "DB"
database_name = "openai-api-prod"
database_id = "your-d1-database-id"

[[env.production.kv_namespaces]]
binding = "CACHE"
id = "your-kv-namespace-id"
preview_id = "your-kv-preview-id"
```

### Database Schema

```sql
-- D1 Database Schema
CREATE TABLE IF NOT EXISTS usage_logs (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  user_id TEXT NOT NULL,
  request_id TEXT NOT NULL,
  model TEXT NOT NULL,
  input_tokens INTEGER NOT NULL,
  output_tokens INTEGER NOT NULL,
  cost REAL NOT NULL,
  latency INTEGER NOT NULL,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_usage_logs_user_id ON usage_logs(user_id);
CREATE INDEX IF NOT EXISTS idx_usage_logs_created_at ON usage_logs(created_at);

CREATE TABLE IF NOT EXISTS rate_limits (
  id TEXT PRIMARY KEY,
  user_id TEXT NOT NULL,
  requests_count INTEGER DEFAULT 0,
  window_start DATETIME DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_rate_limits_user_id ON rate_limits(user_id);

CREATE TABLE IF NOT EXISTS api_keys (
  id TEXT PRIMARY KEY,
  user_id TEXT NOT NULL,
  key_hash TEXT NOT NULL,
  name TEXT,
  permissions TEXT DEFAULT 'read,write',
  rate_limit INTEGER DEFAULT 1000,
  is_active BOOLEAN DEFAULT TRUE,
  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
  last_used_at DATETIME
);

CREATE INDEX IF NOT EXISTS idx_api_keys_user_id ON api_keys(user_id);
CREATE INDEX IF NOT EXISTS idx_api_keys_key_hash ON api_keys(key_hash);
```

## üöÄ **API Implementation**

### Main Worker Entry Point

```typescript
// src/index.ts
import { Hono } from "hono";
import { cors } from "hono/cors";
import { logger } from "hono/logger";
import { rateLimiter } from "./middleware/rateLimiter";
import { auth } from "./middleware/auth";
import { chatCompletions } from "./routes/chat";
import { models } from "./routes/models";
import { usage } from "./routes/usage";

type Bindings = {
  OPENROUTER_API_KEY: string;
  POSTHOG_API_KEY: string;
  POSTHOG_HOST: string;
  DB: D1Database;
  CACHE: KVNamespace;
  ALLOWED_ORIGINS: string;
};

const app = new Hono<{ Bindings: Bindings }>();

// Middleware
app.use("*", logger());
app.use(
  "*",
  cors({
    origin: (origin, c) => {
      const allowed = c.env.ALLOWED_ORIGINS.split(",");
      return allowed.includes(origin) || origin === undefined;
    },
    allowMethods: ["GET", "POST", "OPTIONS"],
    allowHeaders: ["Content-Type", "Authorization"]
  })
);

app.use("/v1/*", auth);
app.use("/v1/*", rateLimiter);

// Routes
app.route("/v1/chat", chatCompletions);
app.route("/v1/models", models);
app.route("/v1/usage", usage);

// Health check
app.get("/health", (c) => {
  return c.json({ status: "healthy", timestamp: new Date().toISOString() });
});

// 404 handler
app.notFound((c) => {
  return c.json({ error: "Not Found" }, 404);
});

// Error handler
app.onError((err, c) => {
  console.error("Worker error:", err);
  return c.json({ error: "Internal Server Error" }, 500);
});

export default app;
```

### Authentication Middleware

```typescript
// src/middleware/auth.ts
import { createMiddleware } from "hono/factory";
import { HTTPException } from "hono/http-exception";

export const auth = createMiddleware(async (c, next) => {
  const authorization = c.req.header("Authorization");

  if (!authorization || !authorization.startsWith("Bearer ")) {
    throw new HTTPException(401, {
      message: "Missing or invalid authorization header"
    });
  }

  const apiKey = authorization.slice(7); // Remove 'Bearer ' prefix

  // Validate API key against database
  const keyRecord = await c.env.DB.prepare(
    "SELECT user_id, permissions, rate_limit, is_active FROM api_keys WHERE key_hash = ? AND is_active = TRUE"
  )
    .bind(await hashApiKey(apiKey))
    .first();

  if (!keyRecord) {
    throw new HTTPException(401, { message: "Invalid API key" });
  }

  // Update last used timestamp
  await c.env.DB.prepare(
    "UPDATE api_keys SET last_used_at = CURRENT_TIMESTAMP WHERE key_hash = ?"
  )
    .bind(await hashApiKey(apiKey))
    .run();

  // Set user context
  c.set("userId", keyRecord.user_id);
  c.set("permissions", keyRecord.permissions.split(","));
  c.set("rateLimit", keyRecord.rate_limit);

  await next();
});

async function hashApiKey(apiKey: string): Promise<string> {
  const encoder = new TextEncoder();
  const data = encoder.encode(apiKey);
  const hashBuffer = await crypto.subtle.digest("SHA-256", data);
  const hashArray = Array.from(new Uint8Array(hashBuffer));
  return hashArray.map((b) => b.toString(16).padStart(2, "0")).join("");
}
```

### Rate Limiting Middleware

```typescript
// src/middleware/rateLimiter.ts
import { createMiddleware } from "hono/factory";
import { HTTPException } from "hono/http-exception";

export const rateLimiter = createMiddleware(async (c, next) => {
  const userId = c.get("userId");
  const userRateLimit = c.get("rateLimit") || 60;
  const windowMs = 60 * 1000; // 1 minute
  const now = Date.now();
  const windowStart = now - windowMs;

  // Get current rate limit data
  const rateLimitKey = `rate_limit:${userId}:${Math.floor(now / windowMs)}`;
  const currentCount = await c.env.CACHE.get(rateLimitKey);

  if (currentCount && parseInt(currentCount) >= userRateLimit) {
    throw new HTTPException(429, {
      message: "Rate limit exceeded",
      res: new Response(
        JSON.stringify({
          error: {
            message: "Rate limit exceeded. Please try again later.",
            type: "rate_limit_exceeded",
            code: "rate_limit_exceeded"
          }
        }),
        {
          status: 429,
          headers: {
            "Content-Type": "application/json",
            "X-RateLimit-Limit": userRateLimit.toString(),
            "X-RateLimit-Remaining": "0",
            "X-RateLimit-Reset": Math.ceil((now + windowMs) / 1000).toString()
          }
        }
      )
    });
  }

  // Increment counter
  const newCount = (parseInt(currentCount || "0") + 1).toString();
  await c.env.CACHE.put(rateLimitKey, newCount, { expirationTtl: 60 });

  // Set rate limit headers
  c.header("X-RateLimit-Limit", userRateLimit.toString());
  c.header(
    "X-RateLimit-Remaining",
    (userRateLimit - parseInt(newCount)).toString()
  );
  c.header("X-RateLimit-Reset", Math.ceil((now + windowMs) / 1000).toString());

  await next();
});
```

### Security Implementation

```typescript
// Security headers middleware
export const securityHeaders = createMiddleware(async (c, next) => {
  await next();

  c.header("X-Content-Type-Options", "nosniff");
  c.header("X-Frame-Options", "DENY");
  c.header("X-XSS-Protection", "1; mode=block");
  c.header("Referrer-Policy", "strict-origin-when-cross-origin");
  c.header("Content-Security-Policy", "default-src 'self'");
});

// Input validation
import { z } from "zod";

const chatCompletionSchema = z.object({
  model: z.string().optional(),
  messages: z
    .array(
      z.object({
        role: z.enum(["system", "user", "assistant"]),
        content: z.string().max(32000) // Prevent extremely large inputs
      })
    )
    .min(1)
    .max(100), // Limit conversation length
  temperature: z.number().min(0).max(2).optional(),
  max_tokens: z.number().min(1).max(8192).optional(),
  stream: z.boolean().optional(),
  tools: z.array(z.any()).optional()
});

export function validateChatCompletion(data: unknown) {
  return chatCompletionSchema.parse(data);
}
```

### Performance Optimization

```typescript
// Response caching
export async function getCachedResponse(
  cacheKey: string,
  cache: KVNamespace
): Promise<any> {
  const cached = await cache.get(cacheKey);
  return cached ? JSON.parse(cached) : null;
}

export async function setCachedResponse(
  cacheKey: string,
  response: any,
  cache: KVNamespace,
  ttl: number = 300
): Promise<void> {
  await cache.put(cacheKey, JSON.stringify(response), { expirationTtl: ttl });
}

// Generate cache key for deterministic requests
export async function generateCacheKey(
  messages: any[],
  model: string,
  temperature: number
): Promise<string> {
  const content = JSON.stringify({ messages, model, temperature });
  const data = new TextEncoder().encode(content);
  const hash = await crypto.subtle.digest("SHA-256", data);
  const hex = Array.from(new Uint8Array(hash))
    .map((b) => b.toString(16).padStart(2, "0"))
    .join("");
  return `chat:${hex.slice(0, 32)}`;
}

// Connection pooling and retry logic
export async function fetchWithRetry(
  url: string,
  options: RequestInit,
  maxRetries: number = 3
): Promise<Response> {
  let lastError: Error;

  for (let i = 0; i < maxRetries; i++) {
    try {
      const controller = new AbortController();
      const timer = setTimeout(() => controller.abort(), 30_000);
      const response = await fetch(url, {
        ...options,
        signal: controller.signal
      }).finally(() => clearTimeout(timer));

      if (response.ok || response.status < 500) {
        return response;
      }

      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    } catch (error) {
      lastError = error as Error;

      if (i < maxRetries - 1) {
        // Exponential backoff
        await new Promise((resolve) =>
          setTimeout(resolve, Math.pow(2, i) * 1000)
        );
      }
    }
  }

  throw lastError;
}
```

## üß™ **Testing Strategy**

### Unit Tests

```typescript
// tests/chat.test.ts
import { describe, it, expect, beforeEach } from "vitest";
import { testClient } from "hono/testing";
import app from "../src/index";

describe("Chat Completions API", () => {
  const client = testClient(app);

  beforeEach(() => {
    // Mock environment variables
    process.env.OPENROUTER_API_KEY = "test-key";
    process.env.POSTHOG_API_KEY = "test-posthog-key";
  });

  it("should return 401 without authorization", async () => {
    const res = await client.v1.chat.completions.$post({
      json: {
        model: "anthropic/claude-3.5-sonnet",
        messages: [{ role: "user", content: "Hello" }]
      }
    });

    expect(res.status).toBe(401);
  });

  it("should validate request body", async () => {
    const res = await client.v1.chat.completions.$post({
      json: { messages: [] }, // Invalid: empty messages
      headers: { Authorization: "Bearer test-api-key" }
    });

    expect(res.status).toBe(400);
  });

  it("should handle streaming requests", async () => {
    const res = await client.v1.chat.completions.$post({
      json: {
        model: "anthropic/claude-3.5-sonnet",
        messages: [{ role: "user", content: "Hello" }],
        stream: true
      },
      headers: { Authorization: "Bearer test-api-key" }
    });

    expect(res.headers.get("content-type")).toContain("text/event-stream");
  });
});
```

### Integration Tests

```typescript
// tests/integration.test.ts
import { describe, it, expect } from "vitest";
import { PostHog } from "posthog-node";

describe("Integration Tests", () => {
  it("should track events in PostHog", async () => {
    const posthog = new PostHog("test-key", {
      host: "https://app.posthog.com"
    });

    await posthog.capture({
      distinctId: "test-user",
      event: "$ai_generation",
      properties: {
        $ai_model: "anthropic/claude-3.5-sonnet",
        $ai_input_tokens: 10,
        $ai_output_tokens: 20,
        $ai_cost: 0.001
      }
    });

    await posthog.shutdown();
    expect(true).toBe(true); // Verify no errors thrown
  });

  it("should connect to OpenRouter API", async () => {
    const response = await fetch("https://openrouter.ai/api/v1/models", {
      headers: {
        Authorization: `Bearer ${process.env.OPENROUTER_API_KEY}`
      }
    });

    expect(response.ok).toBe(true);
    const data = await response.json();
    expect(Array.isArray(data.data)).toBe(true);
  });
});
```

### Load Testing

```typescript
// tests/load.test.ts (executed by k6, not Vitest)
import http from "k6/http";
import { check } from "k6";

export const options = {
  stages: [
    { duration: "2m", target: 100 }, // Ramp up
    { duration: "5m", target: 100 }, // Stay at 100 users
    { duration: "2m", target: 200 }, // Ramp up to 200 users
    { duration: "5m", target: 200 }, // Stay at 200 users
    { duration: "2m", target: 0 } // Ramp down
  ],
  thresholds: {
    http_req_duration: ["p(95)<2000"], // 95% of requests under 2s
    http_req_failed: ["rate<0.1"] // Error rate under 10%
  }
};

export default function () {
  const payload = JSON.stringify({
    model: "anthropic/claude-3.5-sonnet",
    messages: [{ role: "user", content: "Hello, how are you?" }],
    max_tokens: 100
  });

  const params = {
    headers: {
      "Content-Type": "application/json",
      Authorization: "Bearer test-api-key"
    }
  };

  const response = http.post(
    "https://api.wemake.cx/v1/chat/completions",
    payload,
    params
  );

  check(response, {
    "status is 200": (r) => r.status === 200,
    "response time < 2000ms": (r) => r.timings.duration < 2000
  });
}
```

## üì¶ **Deployment Guide**

### Local Development Setup

```sh
# Clone repository
git clone https://github.com/WeMake-AI/platform.git
cd platform/workers/openai-api

# Install dependencies
bun install

# Setup environment variables
cp .dev.vars.example .dev.vars
# Edit .dev.vars with your API keys

# Run development server
bun run dev

# Run tests
bun run test
bun run test:integration
bun run test:load
```

### Production Deployment

```sh
# Build and deploy to Cloudflare Workers
bun run build
bun run deploy:staging
bun run deploy:production

# Setup database
wrangler d1 create openai-api-prod
wrangler d1 execute openai-api-prod --file=./schema.sql

# Setup KV namespace
wrangler kv:namespace create "CACHE"
wrangler kv:namespace create "CACHE" --preview

# Configure secrets
wrangler secret put OPENROUTER_API_KEY
wrangler secret put POSTHOG_API_KEY
```

### CI/CD Pipeline

```yaml
# .github/workflows/deploy.yml
name: Deploy OpenAI API Worker

on:
  push:
    branches: [main]
    paths: ["workers/openai-api/**"]
  pull_request:
    paths: ["workers/openai-api/**"]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: oven-sh/setup-bun@v1

      - name: Install dependencies
        run: bun install
        working-directory: workers/openai-api

      - name: Run tests
        run: |
          bun run test
          bun run test:integration
        working-directory: workers/openai-api
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          POSTHOG_API_KEY: ${{ secrets.POSTHOG_API_KEY }}

  deploy-staging:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      - uses: oven-sh/setup-bun@v1

      - name: Deploy to staging
        run: bun run deploy:staging
        working-directory: workers/openai-api
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}

  deploy-production:
    needs: deploy-staging
    runs-on: ubuntu-latest
    environment: production
    steps:
      - uses: actions/checkout@v4
      - uses: oven-sh/setup-bun@v1

      - name: Deploy to production
        run: bun run deploy:production
        working-directory: workers/openai-api
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
```

## üîß **Troubleshooting**

### Common Issues

#### 1\. OpenRouter API Errors

```typescript
// Error handling for OpenRouter issues
if (response.status === 429) {
  // Rate limited by OpenRouter
  const retryAfter = response.headers.get("retry-after");
  throw new Error(
    `OpenRouter rate limit exceeded. Retry after ${retryAfter} seconds`
  );
}

if (response.status === 402) {
  // Insufficient credits
  throw new Error("OpenRouter account has insufficient credits");
}

if (response.status === 400) {
  // Invalid request
  const error = await response.json();
  throw new Error(
    `OpenRouter API error: ${error.error?.message || "Invalid request"}`
  );
}
```

#### 2\. PostHog Connection Issues

```typescript
// PostHog error handling
try {
  await posthog.capture(event);
} catch (error) {
  console.error("PostHog tracking failed:", error);
  // Continue execution - don't fail the request for analytics
}
```

#### 3\. Database Connection Issues

```typescript
// D1 database error handling
try {
  const result = await env.DB.prepare(query)
    .bind(...params)
    .run();
  return result;
} catch (error) {
  console.error("Database error:", error);

  if (error.message.includes("no such table")) {
    throw new Error("Database not properly initialized. Run migrations.");
  }

  throw new Error("Database operation failed");
}
```

### Monitoring and Alerts

```typescript
// Health check endpoint with detailed status
app.get("/health", async (c) => {
  const checks = {
    timestamp: new Date().toISOString(),
    status: "healthy",
    checks: {
      database: "unknown",
      openrouter: "unknown",
      posthog: "unknown"
    }
  };

  // Check database
  try {
    await c.env.DB.prepare("SELECT 1").first();
    checks.checks.database = "healthy";
  } catch (error) {
    checks.checks.database = "unhealthy";
    checks.status = "degraded";
  }

  // Check OpenRouter
  try {
    const response = await fetch("https://openrouter.ai/api/v1/models", {
      headers: { Authorization: `Bearer ${c.env.OPENROUTER_API_KEY}` }
    });
    checks.checks.openrouter = response.ok ? "healthy" : "unhealthy";
  } catch (error) {
    checks.checks.openrouter = "unhealthy";
    checks.status = "degraded";
  }

  // Check PostHog
  try {
    const posthog = new PostHog(c.env.POSTHOG_API_KEY);
    await posthog.capture({
      distinctId: "health-check",
      event: "$health_check",
      properties: { timestamp: checks.timestamp }
    });
    checks.checks.posthog = "healthy";
    await posthog.shutdown();
  } catch (error) {
    checks.checks.posthog = "unhealthy";
    // PostHog issues don't affect core functionality
  }

  const statusCode = checks.status === "healthy" ? 200 : 503;
  return c.json(checks, statusCode);
});
```

### Performance Optimization Tips

1. **Enable Response Caching**
   - Cache deterministic requests for 5-15 minutes
   - Use conversation context as cache key
   - Implement cache invalidation for user preferences
2. **Optimize Database Queries**
   - Use prepared statements
   - Add proper indexes
   - Implement connection pooling
3. **Monitor Resource Usage**
   - Track CPU time and memory usage
   - Monitor request latency
   - Set up alerts for error rates
4. **Implement Circuit Breakers**
   - Fail fast when OpenRouter is down
   - Implement graceful degradation
   - Use fallback models when primary fails

## üìö **Additional Resources**

### API Documentation

- **OpenAPI 3.0 Specification**: Complete API schema with interactive
  documentation
- **Integration Examples**: Code samples for popular frameworks (React, Vue,
  Node.js, Python)
- **SDK Documentation**: AI SDK 5 integration patterns and best practices

### External Documentation

- [OpenRouter API Documentation](https://openrouter.ai/docs)
- [PostHog LLM Observability Guide](https://posthog.com/docs/ai-engineering)
- [AI SDK 5 Documentation](https://sdk.vercel.ai/docs)
- [Cloudflare Workers Documentation](https://developers.cloudflare.com/workers/)
- [Cloudflare D1 Database](https://developers.cloudflare.com/d1/)
- [Cloudflare KV Storage](https://developers.cloudflare.com/kv/)

### Community and Support

- **GitHub Repository**:
  [WeMake Platform](https://github.com/WeMake-AI/platform)
- **Issue Tracking**: Report bugs and feature requests
- **Discussions**: Community support and best practices
- **Contributing**: Guidelines for contributing to the project

### Key Deliverables Summary

‚úÖ **Complete technical specifications** for production-ready LLM endpoint  
‚úÖ **DevOps-integrated CI/CD pipeline** with automated testing and deployment  
‚úÖ **Comprehensive security framework** with authentication and rate limiting  
‚úÖ **Scalable monitoring infrastructure** with PostHog LLM observability  
‚úÖ **Cloudflare AI Gateway integration** for enhanced caching and analytics  
‚úÖ **Multi-level testing strategy** (unit, integration, load, E2E)  
‚úÖ **Production deployment guide** with troubleshooting and optimization tips
